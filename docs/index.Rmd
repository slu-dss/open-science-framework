---
title: "Making Science More Open"
subtitle: "An Introduction to the Open Science Framework"
author: "Chris Prener"
date: "April 2nd, 2019"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

# What We're All About

We're in our **seventh semester** of offering seminars on using `R` for data science.

</br>

We were founded in **2015** by Chris, Christina Garcia and Kelly Lovejoy.

</br>

We are a **collaborative, interdisciplinary** group at Saint Louis University focused on **building community** around open source software and open science.

---

# What We're All About

We `r emojifont::emoji('heart')` `R`, RStudio, open data, and open science!

--

</br>

Our seminars cover *most* of data science workflow from Wickham and Grollman (2016):

```{r add-workflow-image, echo=FALSE}
knitr::include_graphics("assets/workflow.png")
```

---

# The State of Academic Research

Woelfle, Olliaro, and Todd (2011): 

> Academia is associated with the free transmission of data and resources, but in many ways this is no longer how it operates. The scientific community generally works towards common goals by competition between closed groups of scientists and communicates research results through publications relying on pre-publication peer-review. Papers frequently omit some experimental information, or ignore negative results. The delays involved in publication of papers, or reviewing of grants are significant.

---

# The Replication "Crisis"

```{r add-replication-img-1, echo=FALSE, out.width = "80%", fig.align = "center"}
knitr::include_graphics("assets/Baker_2016_1.jpeg")
```

---

# Uneven Responses to the Replication "Crisis"

```{r add-replication-img-2, echo=FALSE, out.width = "90%", fig.align = "center"}
knitr::include_graphics("assets/Baker_2016_2.jpg")
```

---

# Uneven Responses to the Replication "Crisis"

```{r add-replication-img-3, echo=FALSE, out.width = "80%", fig.align = "center"}
knitr::include_graphics("assets/GertlerEtAl_2018_1.jpg")
```

---

# Uneven Responses to the Replication "Crisis"

```{r add-replication-img-4, echo=FALSE, out.width = "60%", fig.align = "center"}
knitr::include_graphics("assets/GertlerEtAl_2018_2.jpg")
```

---

# Open Science to the Rescue?

One problem is that we do not have a cohesive definition of open science. Is it publishing data online? Providing code? Something else?

--

</br>

Gertler, Galiani, and Romero (2018):

> We were able to replicate only a small minority of these papers. Overall, of the 203 studies, 76% published at least one of the 4 files required for replication: the raw data used in the study (32%); the final estimation data set produced after data cleaning and variable manipulation (60%); the data-manipulation code used to convert the raw data to the estimation data (42%, but only 16% had both raw data and usable code that ran); and the estimation code used to produce the final tables and figures (72%).

---

# Open Science to the Rescue!

Possible elements:

1. the raw data used in the study
2. the final estimation data set produced after data cleaning and variable manipulation
3. the data-manipulation code used to convert the raw data to the estimation data
4. the estimation code used to produce the final tables and figures

---

# Open Science to the Rescue!

Possible elements:

1. pre-registered hypotheses
2. protocols and research materials
3. research logs
4. the raw data used in the study
5. the final estimation data set produced after data cleaning and variable manipulation
6. the data-manipulation code used to convert the raw data to the estimation data
7. the estimation code used to produce the final tables and figures

---

# Open Science to the Rescue!

Possible elements:

1. pre-registered hypotheses
2. protocols and research materials
3. research logs
4. the raw data used in the study
5. the final estimation data set produced after data cleaning and variable manipulation
6. the data-manipulation code used to convert the raw data to the estimation data
7. the estimation code used to produce the final tables and figures
8. all code should run on any computer

---

# Open Science to the Rescue!

Possible elements:

1. pre-registered hypotheses
2. protocols and research materials
3. research logs
4. the raw data used in the study
5. the final estimation data set produced after data cleaning and variable manipulation
6. the data-manipulation code used to convert the raw data to the estimation data
7. the estimation code used to produce the final tables and figures
8. all code should run on any computer
9. all of the above materials should be available online in a format that makes them cite-able

---

# Open Science to the Rescue!

Possible elements:

1. pre-registered hypotheses
2. protocols and research materials
3. research logs
4. the raw data used in the study
5. the final estimation data set produced after data cleaning and variable manipulation
6. the data-manipulation code used to convert the raw data to the estimation data
7. the estimation code used to produce the final tables and figures
8. all code should run on any computer
9. all of the above materials should be available online in a format that makes them cite-able
10. results are available without pay wall

---

# How do we do this?

1. Use the Open Science Framework (OSF) for:
    * pre-registration
    * cataloging **all** research materials
    * creating a DOI number to make work cite-able
    * publish pre-prints of findings

---

# How do we do this?

1. Use the Open Science Framework (OSF) for:
    * pre-registration
    * cataloging **all** research materials
    * creating a DOI number to make work cite-able
    * publish pre-prints of findings
2. If you want to make data cite-able independent of a single project, use Zenodo, figshare, or another service that creates DOI numbers for data sets

---

# How do we do this?

1. Use the Open Science Framework (OSF) for:
    * pre-registration
    * cataloging **all** research materials
    * creating a DOI number to make work cite-able
    * publish pre-prints of findings
2. If you want to make data cite-able independent of a single project, use Zenodo, figshare, or another service that creates DOI numbers for data sets
3. If you want the most robust option for executable code, pair OSF with GitHub or Gitlab

```{r quick-analysis, echo=FALSE, include=FALSE}
# load dependencies
library(dplyr)
library(ggplot2)
library(here)
library(janitor)
library(readr)
library(stargazer)

# import and quickly clean data
data <- read_csv(here("data", "original_data.csv")) %>%
  clean_names() %>%
  rename(
    city = city_fuel_efficiency_mpg,
    cyl = number_of_cylinders,
    disp = engine_displacement
  )

# save analysis data
write_csv(data, here("data", "analysis_data.csv"))

# create plot
ggplot(data = data, mapping = aes(disp, city)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(
    title = "Example Plot",
    caption = "Christopher Prener, Ph.D.",
    x = "Engine Displacement",
    y = "City Fuel Efficiency"
    ) -> plot

# save plot
ggsave(here("results", "examplePlot.png"), plot)

# create model
model <- lm(city ~ cyl+disp, data = data)

# create table
stargazer(model, title = "Example Output", type = "html", omit.stat = "rsq", 
          df = FALSE, star.cutoffs= c(0.05, 0.01, 0.001),
          out = here("results", "exampleTable.html"))
```
